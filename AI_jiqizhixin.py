import os
import json
import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ
is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'
if is_github_actions:
    print("åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ")

# ========== ç¯å¢ƒåŠ è½½ (OpenAIç›¸å…³å·²ç§»é™¤) ==========

# ========== åˆå§‹åŒ– (OpenAIç›¸å…³å·²ç§»é™¤) ==========

# ========== å»é‡ç”¨ ==========
# ä½¿ç”¨ HUGO_PROJECT_PATHï¼ˆè‹¥æœªè®¾ç½®åˆ™ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ï¼‰
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', '.') # é»˜è®¤ä¸ºå½“å‰ç›®å½•
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
output_file = os.path.join(base_dir, "jiqizhixin_articles_summarized.jsonl")
# Markdownæ–‡ä»¶ç”Ÿæˆå·²ç§»è‡³AI_summary.py
summarized_titles = set()
if os.path.exists(output_file):
    with open(output_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                summarized_titles.add(data["title"])
            except:
                continue

# ========== æ‘˜è¦ç”Ÿæˆå‡½æ•° (å·²ç§»é™¤) ==========

# ========== ä¸»çˆ¬è™«é€»è¾‘ ==========
async def main():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    async with async_playwright() as p:
        # åœ¨GitHub Actionsä¸­ä½¿ç”¨headlessæ¨¡å¼ï¼Œæœ¬åœ°å¼€å‘å¯è§†åŒ–
        browser = await p.chromium.launch(headless=is_github_actions)
        page = await browser.new_page()
        await page.goto("https://www.jiqizhixin.com/articles", timeout=60000)

        cards = await page.locator("div.article-card").all()

        with open(output_file, "a", encoding="utf-8") as f:
            for i, card in enumerate(cards):
                # æå‰è·å–æ—¶é—´ï¼Œå‡å°‘ä¸å¿…è¦çš„ç‚¹å‡»
                time_text = await card.locator("div.article-card__time").inner_text()
                print(f"[{i + 1}/{len(cards)}] æ£€æŸ¥æ–‡ç« : {time_text}")

                if "å¤©å‰" in time_text or "æœˆå‰" in time_text or "å¹´å‰" in time_text:
                    print("ğŸ›‘ é‡åˆ°è¾ƒæ—©çš„æ–‡ç« ï¼Œåœæ­¢æŠ“å–ã€‚")
                    break

                # è®¾ç½®ç›‘å¬
                try:
                    async with page.expect_response(
                        lambda res: "/api/v4/articles/" in res.url and res.status == 200,
                        timeout=30000  # ç¼©çŸ­ç­‰å¾…æ—¶é—´
                    ) as res_info:
                        await card.click()
                        await page.wait_for_load_state("domcontentloaded") # ç­‰å¾…DOMå³å¯ï¼Œæ— éœ€ç­‰å¾…æ‰€æœ‰èµ„æº
                        response = await res_info.value
                        data = await response.json()
                        # è·å–å½“å‰é¡µé¢çš„URL
                        article_url = page.url

                except Exception as e:
                    print(f"âš ï¸ é¡µé¢åŠ è½½æˆ–APIè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡è¯¥ç¯‡æ–‡ç« : {e}")
                    # å‡ºé”™åï¼Œè¿”å›åˆ—è¡¨é¡µå¹¶é‡æ–°è·å–å¡ç‰‡åˆ—è¡¨ä»¥ä¿è¯çŠ¶æ€åŒæ­¥
                    await page.go_back()
                    await page.wait_for_load_state("domcontentloaded")
                    continue

                title = data.get("title")
                if not title or title in summarized_titles:
                    print(f"â­ï¸ è·³è¿‡å·²å¤„ç†æˆ–æ— æ ‡é¢˜çš„æ–‡ç« : {title}")
                    await page.go_back() # è¿”å›åˆ—è¡¨é¡µ
                    await page.wait_for_timeout(500) # ç­‰å¾…ä¸€ä¸‹
                    continue
                
                # æ ¸å¿ƒæ”¹è¿›ï¼šè§£æHTMLå†…å®¹å¹¶æå–çº¯æ–‡æœ¬
                html_content = data.get("content")
                if not html_content:
                    print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ (content)ï¼Œè·³è¿‡æ–‡ç« : {title}")
                    await page.go_back() # è¿”å›åˆ—è¡¨é¡µ
                    await page.wait_for_timeout(500)
                    continue

                soup = BeautifulSoup(html_content, "html.parser")
                
                # ä¼˜å…ˆå°è¯•æå–ç‰¹å®šæ–‡ç« å†…å®¹å®¹å™¨ï¼Œå¦‚æœå¤±è´¥åˆ™æå–å…¨éƒ¨æ–‡æœ¬
                article_body = soup.find('div', class_='article__content')
                if article_body:
                    content = article_body.get_text(separator="\n", strip=True)
                else:
                    content = soup.get_text(separator="\n", strip=True)

                # AIæ‘˜è¦ç”Ÿæˆå·²ç§»é™¤ï¼Œåªå‡†å¤‡æ•°æ®
                row = {
                    "title": title,
                    "published_at": data.get("published_at"),
                    "url": article_url,
                    "content": content, # æä¾›ç»™ AI_summary.py çš„åŸæ–‡
                }

                # å†™å…¥ JSONL
                f.write(json.dumps(row, ensure_ascii=False) + "\n")
                summarized_titles.add(title)
                
                print(f"âœ… å·²çˆ¬å–æ–‡ç« : {title}")

                # è¿”å›æ–‡ç« åˆ—è¡¨é¡µï¼Œå‡†å¤‡å¤„ç†ä¸‹ä¸€ç¯‡
                await page.go_back()
                # ç­‰å¾…åˆ—è¡¨é¡µåŠ è½½å®Œæˆ
                await page.wait_for_load_state("domcontentloaded")
                await page.wait_for_timeout(1000) # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…è¿‡å¿«æ“ä½œ

        await browser.close()

# è¿è¡Œçˆ¬è™«
if __name__ == "__main__":
    asyncio.run(main())
