import os
import json
import asyncio
import time
import random
from dotenv import load_dotenv
from openai import OpenAI
from playwright.async_api import async_playwright
# SeaTable ç›¸å…³ä¾èµ–å·²ç§»é™¤
from bs4 import BeautifulSoup
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ
is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'
if is_github_actions:
    print("åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ")

# ========== ç¯å¢ƒåŠ è½½ ==========
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")
SEATABLE_API_TOKEN = os.getenv("SEATABLE_API_TOKEN")
SEATABLE_SERVER_URL = os.getenv("SEATABLE_SERVER_URL")

# ========== åˆå§‹åŒ– ==========
# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œæ·»åŠ æ›´å¤šé…ç½®
# åˆ›å»ºå…·æœ‰é‡è¯•åŠŸèƒ½çš„ä¼šè¯
session = requests.Session()
retry_strategy = Retry(
    total=5,  # æœ€å¤šé‡è¯•5æ¬¡
    backoff_factor=1,  # é‡è¯•é—´éš”
    status_forcelist=[429, 500, 502, 503, 504],  # è¿™äº›HTTPçŠ¶æ€ç ä¼šè§¦å‘é‡è¯•
    allowed_methods=["GET", "POST"]  # å…è®¸é‡è¯•çš„HTTPæ–¹æ³•
)
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)

# ä½¿ç”¨é…ç½®å¥½çš„ä¼šè¯åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_API_BASE,
    http_client=session,
    timeout=60.0  # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
)
# å·²ç§»é™¤ SeaTable åˆå§‹åŒ–
table_name = "AIæ‘˜è¦"

# ========== å»é‡ç”¨ ==========
# ä½¿ç”¨ HUGO_PROJECT_PATHï¼ˆè‹¥æœªè®¾ç½®åˆ™ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ï¼‰
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', r'C:\Users\kongg\0')
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
output_file = os.path.join(base_dir, "jiqizhixin_articles_summarized.jsonl")
markdown_file = os.path.join(base_dir, "jiqizhixin_articles_summarized.md")
summarized_titles = set()
if os.path.exists(output_file):
    with open(output_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                summarized_titles.add(data["title"])
            except:
                continue

# ========== æ‘˜è¦ç”Ÿæˆå‡½æ•° ==========
# æ·»åŠ é‡è¯•é€»è¾‘
def call_openai_with_retry(client, model, messages, temperature=0.7, max_retries=5, base_delay=2):
    """ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•è°ƒç”¨OpenAI API"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                timeout=60.0,  # æ˜ç¡®è®¾ç½®è¶…æ—¶æ—¶é—´
                request_timeout=60.0  # è¯·æ±‚è¶…æ—¶
            )
            return response
        except Exception as e:
            if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•
                raise e
            
            # æŒ‡æ•°é€€é¿ç­–ç•¥
            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
            print(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{max_retries})ï¼Œ{delay:.2f}ç§’åé‡è¯•: {e}")
            time.sleep(delay)

async def generate_summaries(title, content):
    try:
        print(f"âœï¸ æ­£åœ¨ç”Ÿæˆæ‘˜è¦: {title}")

        # ä¸­æ–‡æ‘˜è¦ï¼ˆå¸¦é‡è¯•ï¼‰
        messages_zh = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±ç§‘æŠ€ç¼–è¾‘ï¼Œæ“…é•¿æç‚¼å¤æ‚æ–‡ç« çš„æ ¸å¿ƒå†…å®¹ã€‚è¯·ç”¨ç®€æ´ã€ä¸“ä¸šã€å‡†ç¡®çš„è¯­è¨€ï¼Œç”Ÿæˆä¸€æ®µä¸è¶…è¿‡150å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œæ¦‚æ‹¬æ–‡ç« çš„ä¸»è¦è§‚ç‚¹ã€å…³é”®æ•°æ®ä¸ç»“è®ºï¼Œé¿å…ä¸»è§‚è¯„ä»·ï¼Œä¿æŒæ–°é—»æŠ¥é“é£æ ¼ã€‚"},
            {"role": "user", "content": f"ä»¥ä¸‹æ˜¯æ–‡ç« æ­£æ–‡ï¼Œè¯·ä¸ºå…¶æ’°å†™ä¸“ä¸šæ‘˜è¦ï¼š\n\n{content}"},
        ]
        res_zh = call_openai_with_retry(client, "gpt-3.5-turbo", messages_zh, temperature=0.5)
        summary_zh = res_zh.choices[0].message.content.strip()

        # è‹±æ–‡æ‘˜è¦ï¼ˆå¸¦é‡è¯•ï¼‰
        messages_en = [
            {"role": "system", "content": "You are a professional tech journalist with expertise in summarizing complex articles. Please generate a concise and informative summary (no more than 100 words) that captures the article's key points, findings, and implications. Avoid subjective opinions and use a neutral, journalistic tone."},
            {"role": "user", "content": f"Here is the article content. Please write a high-quality English summary:\n\n{content}"},
        ]
        res_en = call_openai_with_retry(client, "gpt-3.5-turbo", messages_en, temperature=0.5)
        summary_en = res_en.choices[0].message.content.strip()

        return summary_zh, summary_en

    except Exception as e:
        print(f"\nâŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {title}\nåŸå› : {e}")
        return None, None


# ========== å¢åŠ è¶…æ—¶é‡è¯•çš„å‡½æ•° ==========
# å·²ç§»é™¤ SeaTable å†™å…¥å‡½æ•°

# ========== ä¸»çˆ¬è™«é€»è¾‘ ==========
async def main():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    os.makedirs(os.path.dirname(markdown_file), exist_ok=True)
    async with async_playwright() as p:
        # åœ¨GitHub Actionsä¸­ä½¿ç”¨headlessæ¨¡å¼ï¼Œæœ¬åœ°å¼€å‘å¯è§†åŒ–
        browser = await p.chromium.launch(headless=is_github_actions)
        page = await browser.new_page()
        await page.goto("https://www.jiqizhixin.com/articles", timeout=60000)

        cards = await page.locator("div.article-card").all()

        with open(output_file, "a", encoding="utf-8") as f, \
             open(markdown_file, "a", encoding="utf-8") as md_f:
            for i, card in enumerate(cards):
                time_text = await card.locator("div.article-card__time").inner_text()
                print(f"â±ï¸ ç¬¬ {i + 1} ç¯‡æ–‡ç« æ—¶é—´: {time_text}")

                if "1å¤©å‰" in time_text:
                    print("ğŸ›‘ é‡åˆ°ã€1å¤©å‰ã€ï¼Œåœæ­¢æŠ“å–ã€‚")
                    break

                # è®¾ç½®ç›‘å¬
                try:
                    async with page.expect_response(
                        lambda res: "/api/v4/articles/" in res.url and res.status == 200,
                        timeout=60000  # åŠ é•¿ç­‰å¾…æ—¶é—´
                    ) as res_info:
                        print("ğŸ–±ï¸ ç‚¹å‡»æ–‡ç« ")
                        await card.click()

                    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                    await page.wait_for_load_state("load")

                    response = await res_info.value
                    data = await response.json()

                except Exception as e:
                    print(f"âš ï¸ é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡è¯¥ç¯‡æ–‡ç« : {e}")
                    await page.goto("https://www.jiqizhixin.com/articles", timeout=60000)
                    cards = await page.locator("div.article-card").all()
                    continue

                title = data.get("title")
                published_at = data.get("published_at")
                html_content = data.get("content")

                if not title or not html_content:
                    continue
                if title in summarized_titles:
                    continue

                # æå–çº¯æ–‡æœ¬
                soup = BeautifulSoup(html_content, "html.parser")
                content = soup.get_text(separator="\n").strip()

                # è°ƒç”¨ AI ç”Ÿæˆæ‘˜è¦
                summary_cn, summary_en = await generate_summaries(title, content)
                if not summary_cn or not summary_en:
                    continue

                row = {
                    "title": title,
                    "published_at": published_at,
                    "content": content,
                    "summary_cn": summary_cn,
                    "summary_en": summary_en,
                }

                # å†™å…¥ JSONL
                f.write(json.dumps(row, ensure_ascii=False) + "\n")
                f.flush()
                summarized_titles.add(title)

                # å†™å…¥ Markdown
                md_f.write(f"## {title}\n")
                md_f.write(f"- å‘å¸ƒæ—¶é—´: {published_at}\n\n")
                md_f.write(f"**ä¸­æ–‡æ‘˜è¦ï¼š**\n\n{summary_cn}\n\n")
                md_f.write(f"**English Summary:**\n\n{summary_en}\n\n")
                md_f.write("---\n\n")
                md_f.flush()

                # å·²ç§»é™¤å†™å…¥ SeaTable çš„é€»è¾‘

                # ç­‰å¾…ä¸€æ®µæ—¶é—´å†å¤„ç†ä¸‹ä¸€ç¯‡
                await page.wait_for_timeout(1000)

        await browser.close()

# è¿è¡Œçˆ¬è™«
asyncio.run(main())
