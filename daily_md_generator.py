import os
import json
import hashlib
import shutil
from datetime import datetime, timedelta
import glob
import re

# è‡ªåŠ¨å®šä½ summarized_articles.jsonl çš„æœ€æ–°æ–‡ä»¶
# ä¼˜å…ˆæŸ¥æ‰¾ AI_summary.py ç”Ÿæˆçš„è·¯å¾„
# å…¼å®¹å¤šå¹³å°

def find_latest_summary_jsonl():
    # ä»ç¯å¢ƒå˜é‡è¯»å–hugoé¡¹ç›®è·¯å¾„
    hugo_project_path = os.getenv('HUGO_PROJECT_PATH', os.getcwd())
    # 1. å…ˆæŸ¥æ‰¾ AI_summary.py é‡Œçš„ base_dir è·¯å¾„
    candidate = os.path.join(hugo_project_path, 'spiders', 'ai_news', 'summarized_articles.jsonl')
    if os.path.exists(candidate):
        return candidate
    # 2. å…¶æ¬¡æŸ¥æ‰¾å½“å‰ç›®å½•åŠå­ç›®å½•ä¸‹æ‰€æœ‰åŒåæ–‡ä»¶ï¼Œå–æœ€æ–°
    files = glob.glob('**/summarized_articles.jsonl', recursive=True)
    if files:
        files = sorted(files, key=lambda x: os.path.getmtime(x), reverse=True)
        return files[0]
    return None

# ä»ç¯å¢ƒå˜é‡è¯»å–hugoé¡¹ç›®è·¯å¾„ï¼Œå¦‚æœæœªè®¾ç½®ï¼Œåˆ™é»˜è®¤ä¸ºç”¨æˆ·æœ¬åœ°çš„ç»å¯¹è·¯å¾„
# åœ¨ GitHub Action ä¸­ï¼Œä½ éœ€è¦è®¾ç½® HUGO_PROJECT_PATH è¿™ä¸ª secret
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', r'C:\Users\kongg\0')

# ç›®æ ‡æ ¹ç›®å½•
# ä¾‹å¦‚ï¼šC:\Users\kongg\0\content\post
# å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
# è¿™é‡Œå‡è®¾ä¸åŸé€»è¾‘ä¸€è‡´
# ä½ å¯ä»¥æ ¹æ®å®é™…Hugoè·¯å¾„ä¿®æ”¹ target_root
#
target_root = os.path.join(hugo_project_path, 'content', 'post')

def safe_filename(name):
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å¤¹å
    return ''.join(c if c.isalnum() or c in '-_.' else '_' for c in name)[:40]

# è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
def get_content_hash(content):
    return hashlib.md5(content.encode('utf-8')).hexdigest()

# è®¡ç®—æ ‡é¢˜çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹ç›¸åŒæ ‡é¢˜çš„æ–‡ç« 
def get_title_hash(title):
    # ç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼Œè½¬ä¸ºå°å†™åè®¡ç®—å“ˆå¸Œå€¼
    normalized_title = ''.join(c.lower() for c in title if c.isalnum())
    return hashlib.md5(normalized_title.encode('utf-8')).hexdigest()

# è·å–å‰ä¸€å¤©çš„æ—¥æœŸç›®å½•
def get_previous_day_folder():
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    yesterday_safe = yesterday.replace('-', '_')
    return os.path.join(target_root, yesterday_safe)

# æ”¶é›†å·²å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯ï¼ŒåŒ…æ‹¬å†…å®¹å“ˆå¸Œå’Œæ ‡é¢˜å“ˆå¸Œ
def collect_existing_articles_info(days=7):
    content_hash_set = set()  # å†…å®¹å“ˆå¸Œé›†åˆ
    title_hash_map = {}       # æ ‡é¢˜å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„çš„æ˜ å°„
    
    # éå†æœ€è¿‘å‡ å¤©çš„æ–‡ä»¶å¤¹
    for day_offset in range(0, days+1):  # åŒ…æ‹¬ä»Šå¤©(0)
        day_date = (datetime.now() - timedelta(days=day_offset)).strftime('%Y-%m-%d')
        day_folder = os.path.join(target_root, day_date.replace('-', '_'))
        
        if not os.path.exists(day_folder):
            continue
            
        # æŸ¥æ‰¾æ‰€æœ‰index.mdæ–‡ä»¶
        for root, dirs, files in os.walk(day_folder):
            for file in files:
                if file.lower() == 'index.md':
                    index_path = os.path.join(root, file)
                    try:
                        with open(index_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            # æå–æ ‡é¢˜
                            title_match = re.search(r"title\s*=\s*'([^']*)'", content)
                            if title_match:
                                title = title_match.group(1)
                                title_hash = get_title_hash(title)
                                # ä¿å­˜æ ‡é¢˜å“ˆå¸Œå’Œå¯¹åº”çš„æ–‡ä»¶å¤¹è·¯å¾„
                                title_hash_map[title_hash] = os.path.dirname(index_path)
                            
                            # æå–æ­£æ–‡éƒ¨åˆ†ï¼ˆå»é™¤front matterï¼‰
                            content_match = re.search(r'^\+\+\+\n.*?\+\+\+\n(.*)', content, re.DOTALL)
                            if content_match:
                                content_body = content_match.group(1)
                                content_hash = get_content_hash(content_body)
                                content_hash_set.add(content_hash)
                    except Exception as e:
                        print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {index_path}: {e}")
    
    print(f"å·²æ”¶é›† {len(content_hash_set)} ä¸ªç°æœ‰å†…å®¹å“ˆå¸Œå€¼å’Œ {len(title_hash_map)} ä¸ªæ ‡é¢˜å“ˆå¸Œå€¼ç”¨äºå»é‡")
    return content_hash_set, title_hash_map

# æ£€æŸ¥å½“å¤©æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å­˜åœ¨é‡å¤æ–‡ç« ï¼Œå¦‚æœæœ‰åˆ™åˆ é™¤
def remove_duplicates_in_today_folder(today_folder):
    if not os.path.exists(today_folder):
        return 0
        
    content_hashes = {}  # å†…å®¹å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„
    title_hashes = {}    # æ ‡é¢˜å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„
    duplicates = []      # è¦åˆ é™¤çš„é‡å¤æ–‡ä»¶å¤¹
    
    # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰æ–‡ç« çš„å“ˆå¸Œå€¼
    for item in os.listdir(today_folder):
        item_path = os.path.join(today_folder, item)
        if os.path.isdir(item_path):
            index_path = os.path.join(item_path, 'index.md')
            if os.path.exists(index_path):
                try:
                    with open(index_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # æå–æ ‡é¢˜
                        title_match = re.search(r"title\s*=\s*'([^']*)'", content)
                        if title_match:
                            title = title_match.group(1)
                            title_hash = get_title_hash(title)
                            
                            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
                            if title_hash in title_hashes:
                                duplicates.append(item_path)
                                print(f"âš ï¸ å‘ç°é‡å¤æ ‡é¢˜: {title}")
                                continue
                            title_hashes[title_hash] = item_path
                        
                        # æå–æ­£æ–‡éƒ¨åˆ†
                        content_match = re.search(r'^\+\+\+\n.*?\+\+\+\n(.*)', content, re.DOTALL)
                        if content_match:
                            content_body = content_match.group(1)
                            content_hash = get_content_hash(content_body)
                            
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                            if content_hash in content_hashes:
                                duplicates.append(item_path)
                                print(f"âš ï¸ å‘ç°é‡å¤å†…å®¹: {title}")
                                continue
                            content_hashes[content_hash] = item_path
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {index_path}: {e}")
    
    # ç¬¬äºŒéï¼šåˆ é™¤é‡å¤çš„æ–‡ä»¶å¤¹
    for dup_path in duplicates:
        try:
            shutil.rmtree(dup_path)
            print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤æ–‡ä»¶å¤¹: {os.path.basename(dup_path)}")
        except Exception as e:
            print(f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥ {dup_path}: {e}")
    
    return len(duplicates)

def generate_daily_news_folders():
    today = datetime.now().strftime('%Y-%m-%d')
    today_safe = today.replace('-', '_')
    today_folder = os.path.join(target_root, today_safe)
    os.makedirs(today_folder, exist_ok=True)
    
    # å…ˆæ¸…ç†å½“å¤©æ–‡ä»¶å¤¹ä¸­çš„é‡å¤æ–‡ç« 
    removed_count = remove_duplicates_in_today_folder(today_folder)
    if removed_count > 0:
        print(f"å·²åˆ é™¤å½“å¤©æ–‡ä»¶å¤¹ä¸­çš„ {removed_count} ä¸ªé‡å¤æ–‡ç« ")

    # æ”¶é›†å·²å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯
    existing_content_hashes, existing_title_hash_map = collect_existing_articles_info()

    summary_jsonl = find_latest_summary_jsonl()
    if not summary_jsonl or not os.path.exists(summary_jsonl):
        print('æœªæ‰¾åˆ° summarized_articles.jsonlï¼Œè¯·å…ˆè¿è¡Œ AI_summary.py')
        return
    with open(summary_jsonl, 'r', encoding='utf-8') as f:
        articles = [json.loads(line) for line in f if line.strip()]

    # è®°å½•å¤„ç†ç»“æœ
    total_articles = len(articles)
    skipped_articles = 0
    generated_articles = 0
    duplicate_title_count = 0

    # å½“å‰å¤„ç†çš„æ–‡ç« å†…å®¹å“ˆå¸Œé›†åˆï¼Œç”¨äºé˜²æ­¢å½“å¤©å†…é‡å¤
    today_content_hashes = set()
    today_title_hashes = set()

    # æ¯æ¡æ–°é—»ä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼Œå†…æœ‰index.md
    for idx, article in enumerate(articles):
        title = article.get('title', f'news_{idx+1}')
        summary = article.get('summary', '')
        url = article.get('url', '')
        original_content = article.get('original_content', '')
        
        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
        title_hash = get_title_hash(title)
        if title_hash in existing_title_hash_map:
            duplicate_folder = existing_title_hash_map[title_hash]
            print(f"â­ï¸ è·³è¿‡é‡å¤æ ‡é¢˜: {title}")
            print(f"   å·²å­˜åœ¨äº: {duplicate_folder}")
            skipped_articles += 1
            continue
        
        if title_hash in today_title_hashes:
            print(f"â­ï¸ è·³è¿‡å½“å¤©é‡å¤æ ‡é¢˜: {title}")
            skipped_articles += 1
            continue
        
        # ç”Ÿæˆæ–‡ç« å†…å®¹
        head = [
            "+++\n",
            f"date = '{datetime.now().strftime('%Y-%m-%dT%H:%M:%S+08:00')}'\n",
            "draft = false\n",
            f"title = '{title}'\n",
        ]
        
        # å¦‚æœæœ‰URLï¼Œæ·»åŠ åˆ°front matterï¼Œä½†ä¸ä½¿ç”¨å®Œæ•´URLï¼Œåªä¿ç•™ç›¸å¯¹è·¯å¾„
        if url:
            # ç§»é™¤åè®®éƒ¨åˆ†ï¼Œé¿å…Hugoæ„å»ºé”™è¯¯
            cleaned_url = url.replace('http://', '').replace('https://', '')
            head.append(f"url = '{cleaned_url}'\n")
            
        head.append("+++\n\n")
        
        # å†…å®¹éƒ¨åˆ†ï¼šAIæ‘˜è¦ä½œä¸ºé¦–é¡µæ˜¾ç¤ºå†…å®¹
        content_body = summary + '\n\n'
        
        # æ·»åŠ åŸæ–‡é“¾æ¥å’ŒåŸæ–‡æ‘˜è¦ï¼Œä½†ä½¿ç”¨çº¯æ–‡æœ¬å½¢å¼è€ŒéMarkdowné“¾æ¥
        if url:
            content_body += f"## åŸæ–‡é“¾æ¥\n\n{title}\n{url}\n\n"
        
        if original_content:
            content_body += f"## åŸæ–‡æ‘˜è¦\n\n{original_content}\n\n"
            
        # æ£€æŸ¥å†…å®¹æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
        content_hash = get_content_hash(content_body)
        if content_hash in existing_content_hashes:
            print(f"â­ï¸ è·³è¿‡é‡å¤å†…å®¹: {title}")
            skipped_articles += 1
            continue
        
        if content_hash in today_content_hashes:
            print(f"â­ï¸ è·³è¿‡å½“å¤©é‡å¤å†…å®¹: {title}")
            skipped_articles += 1
            continue
        
        # å†…å®¹ä¸é‡å¤ï¼Œç”Ÿæˆæ–‡ä»¶å¤¹å’Œindex.md
        folder_name = f"{idx+1:02d}_{safe_filename(title)}"
        news_folder = os.path.join(today_folder, folder_name)
        os.makedirs(news_folder, exist_ok=True)
        index_md_path = os.path.join(news_folder, 'index.md')
        
        # åˆæˆå®Œæ•´å†…å®¹
        full_content = ''.join(head) + content_body
        
        # å†™å…¥æ–‡ä»¶
        with open(index_md_path, 'w', encoding='utf-8') as f:
            f.write(full_content)
            
        # è®°å½•å·²ç”Ÿæˆ
        generated_articles += 1
        today_content_hashes.add(content_hash)  # é˜²æ­¢å½“å¤©å†…é‡å¤
        today_title_hashes.add(title_hash)      # é˜²æ­¢å½“å¤©å†…é‡å¤æ ‡é¢˜

    print(f'æ€»è®¡: {total_articles} ç¯‡æ–‡ç« ')
    print(f'å·²ç”Ÿæˆ: {generated_articles} ç¯‡')
    print(f'å·²è·³è¿‡: {skipped_articles} ç¯‡(é‡å¤å†…å®¹æˆ–æ ‡é¢˜)')
    print(f'å…¨éƒ¨å½’ç±»äº: {today_folder}')

if __name__ == '__main__':
    generate_daily_news_folders() 