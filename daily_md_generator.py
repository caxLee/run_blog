import os
import json
import hashlib
import shutil
from datetime import datetime, timedelta
import glob
import re
import sys
import pytz

# --- æ™ºèƒ½è·¯å¾„é…ç½® ---
# é€šè¿‡è„šæœ¬è‡ªèº«ä½ç½®åŠ¨æ€è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼Œä¸å†ä¾èµ–ç¯å¢ƒå˜é‡
# __file__ æ˜¯è„šæœ¬è‡ªèº«çš„ç»å¯¹è·¯å¾„
# os.path.dirname(__file__) æ˜¯è„šæœ¬æ‰€åœ¨çš„ç›®å½• (e.g., /path/to/project/blogdata)
# os.path.dirname(...) å†ä¸€æ¬¡ï¼Œå°±æ˜¯é¡¹ç›®çš„æ ¹ç›®å½• (e.g., /path/to/project)
hugo_project_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
print(f"âœ… [daily_md_generator.py] åŠ¨æ€è¯†åˆ« Hugo é¡¹ç›®æ ¹ç›®å½•: {hugo_project_path}")

TARGET_TIMEZONE = pytz.timezone("Asia/Shanghai")
print(f"ğŸ•’ ä½¿ç”¨ç›®æ ‡æ—¶åŒº: {TARGET_TIMEZONE}")
# --- è·¯å¾„é…ç½®ç»“æŸ ---

# è‡ªåŠ¨å®šä½ summarized_articles.jsonl çš„æœ€æ–°æ–‡ä»¶
# ä¼˜å…ˆæŸ¥æ‰¾ AI_summary.py ç”Ÿæˆçš„è·¯å¾„
# å…¼å®¹å¤šå¹³å°

def find_latest_summary_jsonl():
    # åœ¨å¤šä»“åº“æ£€å‡ºçš„ Actions ç¯å¢ƒä¸­, è·¯å¾„å¿…é¡»æ˜¯ç¡®å®šçš„
    # å‡è®¾ 'scraper_tool' å’Œ 'hugo_source' åœ¨åŒä¸€ä¸ªå·¥ä½œåŒºæ ¹ç›®å½•ä¸‹
    # å¹¶ä¸” AI_summary.py å·²ç»å°†æ–‡ä»¶ç”Ÿæˆåˆ°äº†æ­£ç¡®çš„ä½ç½®
    # è¿™ä¸ªä½ç½®åº”è¯¥æ˜¯ç”± HUGO_PROJECT_PATH æ¨æ–­å‡ºæ¥çš„
    summary_path = os.path.join(hugo_project_path, 'spiders', 'ai_news', 'summarized_articles.jsonl')
    
    if os.path.exists(summary_path):
        return summary_path
    
    # ä½œä¸ºå¤‡é€‰ï¼Œåœ¨å½“å‰å·¥å…·ç›®å½•é‡Œæ‰¾
    if os.path.exists('summarized_articles.jsonl'):
        return 'summarized_articles.jsonl'
        
    print(f"âš ï¸ è­¦å‘Š: åœ¨é¢„è®¾è·¯å¾„ {summary_path} ä¸­æœªæ‰¾åˆ°æ‘˜è¦æ–‡ä»¶ã€‚")
    return None

# ä»ç¯å¢ƒå˜é‡è¯»å–hugoé¡¹ç›®è·¯å¾„ï¼Œå¦‚æœæœªè®¾ç½®ï¼Œåˆ™è„šæœ¬ä¼šæå‰é€€å‡º
# hugo_project_path = os.getenv('HUGO_PROJECT_PATH') # å·²åœ¨é¡¶éƒ¨å®šä¹‰å’Œæ£€æŸ¥

# ç›®æ ‡æ ¹ç›®å½•
# ä¾‹å¦‚ï¼šC:\Users\kongg\0\content\post
# å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
# è¿™é‡Œå‡è®¾ä¸åŸé€»è¾‘ä¸€è‡´
# ä½ å¯ä»¥æ ¹æ®å®é™…Hugoè·¯å¾„ä¿®æ”¹ target_root
#
target_root = os.path.join(hugo_project_path, 'content', 'post')

# ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
os.makedirs(target_root, exist_ok=True)
print(f"ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨: {target_root}")

def safe_filename(name):
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å¤¹å
    return ''.join(c if c.isalnum() or c in '-_.' else '_' for c in name)[:40]

# è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
def get_content_hash(content):
    return hashlib.md5(content.encode('utf-8')).hexdigest()

# è®¡ç®—æ ‡é¢˜çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹ç›¸åŒæ ‡é¢˜çš„æ–‡ç« 
def get_title_hash(title):
    # ç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼Œè½¬ä¸ºå°å†™åè®¡ç®—å“ˆå¸Œå€¼
    normalized_title = ''.join(c.lower() for c in title if c.isalnum())
    return hashlib.md5(normalized_title.encode('utf-8')).hexdigest()

# è·å–å‰ä¸€å¤©çš„æ—¥æœŸç›®å½•
def get_previous_day_folder():
    yesterday = (datetime.now(TARGET_TIMEZONE) - timedelta(days=1)).strftime('%Y-%m-%d')
    yesterday_safe = yesterday.replace('-', '_')
    return os.path.join(target_root, yesterday_safe)

# æ”¶é›†å·²å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯ï¼ŒåŒ…æ‹¬å†…å®¹å“ˆå¸Œå’Œæ ‡é¢˜å“ˆå¸Œ
def collect_existing_articles_info(days=7):
    content_hash_set = set()  # å†…å®¹å“ˆå¸Œé›†åˆ
    title_hash_map = {}       # æ ‡é¢˜å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„çš„æ˜ å°„
    
    # éå†æœ€è¿‘å‡ å¤©çš„æ–‡ä»¶å¤¹
    for day_offset in range(0, days+1):  # åŒ…æ‹¬ä»Šå¤©(0)
        day_date = (datetime.now(TARGET_TIMEZONE) - timedelta(days=day_offset)).strftime('%Y-%m-%d')
        day_folder = os.path.join(target_root, day_date.replace('-', '_'))
        
        if not os.path.exists(day_folder):
            continue
            
        # æŸ¥æ‰¾æ‰€æœ‰index.mdæ–‡ä»¶
        for root, dirs, files in os.walk(day_folder):
            for file in files:
                if file.lower() == 'index.md':
                    index_path = os.path.join(root, file)
                    try:
                        with open(index_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            # æå–æ ‡é¢˜
                            title_match = re.search(r"title\s*=\s*'([^']*)'", content)
                            if title_match:
                                title = title_match.group(1)
                                title_hash = get_title_hash(title)
                                # ä¿å­˜æ ‡é¢˜å“ˆå¸Œå’Œå¯¹åº”çš„æ–‡ä»¶å¤¹è·¯å¾„
                                title_hash_map[title_hash] = os.path.dirname(index_path)
                            
                            # æå–æ­£æ–‡éƒ¨åˆ†ï¼ˆå»é™¤front matterï¼‰
                            content_match = re.search(r'^\+\+\+\n.*?\+\+\+\n(.*)', content, re.DOTALL)
                            if content_match:
                                content_body = content_match.group(1)
                                content_hash = get_content_hash(content_body)
                                content_hash_set.add(content_hash)
                    except Exception as e:
                        print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {index_path}: {e}")
    
    print(f"å·²æ”¶é›† {len(content_hash_set)} ä¸ªç°æœ‰å†…å®¹å“ˆå¸Œå€¼å’Œ {len(title_hash_map)} ä¸ªæ ‡é¢˜å“ˆå¸Œå€¼ç”¨äºå»é‡")
    return content_hash_set, title_hash_map

# æ£€æŸ¥å½“å¤©æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å­˜åœ¨é‡å¤æ–‡ç« ï¼Œå¦‚æœæœ‰åˆ™åˆ é™¤
def remove_duplicates_in_today_folder(today_folder):
    if not os.path.exists(today_folder):
        return 0
        
    content_hashes = {}  # å†…å®¹å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„
    title_hashes = {}    # æ ‡é¢˜å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„
    duplicates = []      # è¦åˆ é™¤çš„é‡å¤æ–‡ä»¶å¤¹
    
    # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰æ–‡ç« çš„å“ˆå¸Œå€¼
    for item in os.listdir(today_folder):
        item_path = os.path.join(today_folder, item)
        if os.path.isdir(item_path):
            index_path = os.path.join(item_path, 'index.md')
            if os.path.exists(index_path):
                try:
                    with open(index_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # æå–æ ‡é¢˜
                        title_match = re.search(r"title\s*=\s*'([^']*)'", content)
                        if title_match:
                            title = title_match.group(1)
                            title_hash = get_title_hash(title)
                            
                            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
                            if title_hash in title_hashes:
                                duplicates.append(item_path)
                                print(f"âš ï¸ å‘ç°é‡å¤æ ‡é¢˜: {title}")
                                continue
                            title_hashes[title_hash] = item_path
                        
                        # æå–æ­£æ–‡éƒ¨åˆ†
                        content_match = re.search(r'^\+\+\+\n.*?\+\+\+\n(.*)', content, re.DOTALL)
                        if content_match:
                            content_body = content_match.group(1)
                            content_hash = get_content_hash(content_body)
                            
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                            if content_hash in content_hashes:
                                duplicates.append(item_path)
                                print(f"âš ï¸ å‘ç°é‡å¤å†…å®¹: {title}")
                                continue
                            content_hashes[content_hash] = item_path
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {index_path}: {e}")
    
    # ç¬¬äºŒéï¼šåˆ é™¤é‡å¤çš„æ–‡ä»¶å¤¹
    for dup_path in duplicates:
        try:
            shutil.rmtree(dup_path)
            print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤æ–‡ä»¶å¤¹: {os.path.basename(dup_path)}")
        except Exception as e:
            print(f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥ {dup_path}: {e}")
    
    return len(duplicates)

def get_next_article_index(folder_path):
    if not os.path.exists(folder_path):
        return 1
    
    max_index = 0
    items = os.listdir(folder_path)
    for item in items:
        if os.path.isdir(os.path.join(folder_path, item)):
            match = re.match(r'^(\d+)_', item)
            if match:
                current_index = int(match.group(1))
                if current_index > max_index:
                    max_index = current_index
    return max_index + 1

def generate_daily_news_folders():
    today = datetime.now(TARGET_TIMEZONE).strftime('%Y-%m-%d')
    today_safe = today.replace('-', '_')
    today_folder = os.path.join(target_root, today_safe)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(today_folder, exist_ok=True)
    print(f"åˆ›å»ºæ–‡ç« ç›®å½•: {today_folder}")

    # è·å–å½“å¤©æ–‡ç« çš„èµ·å§‹åºå·
    next_article_index = get_next_article_index(today_folder)
    print(f"ä»Šå¤©çš„æ–‡ç« å°†ä»åºå· {next_article_index:02d} å¼€å§‹ã€‚")
    
    # å…ˆæ¸…ç†å½“å¤©æ–‡ä»¶å¤¹ä¸­çš„é‡å¤æ–‡ç« 
    removed_count = remove_duplicates_in_today_folder(today_folder)
    if removed_count > 0:
        print(f"å·²åˆ é™¤å½“å¤©æ–‡ä»¶å¤¹ä¸­çš„ {removed_count} ä¸ªé‡å¤æ–‡ç« ")

    # æ”¶é›†å·²å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯
    existing_content_hashes, existing_title_hash_map = collect_existing_articles_info()

    summary_jsonl = find_latest_summary_jsonl()
    if not summary_jsonl or not os.path.exists(summary_jsonl):
        print('æœªæ‰¾åˆ° summarized_articles.jsonlï¼Œè¯·å…ˆè¿è¡Œ AI_summary.py')
        return
    print(f"ä½¿ç”¨æ‘˜è¦æ–‡ä»¶: {summary_jsonl}")
    with open(summary_jsonl, 'r', encoding='utf-8') as f:
        articles = [json.loads(line) for line in f if line.strip()]

    # è®°å½•å¤„ç†ç»“æœ
    total_articles = len(articles)
    skipped_articles = 0
    generated_articles = 0
    duplicate_title_count = 0

    # å½“å‰å¤„ç†çš„æ–‡ç« å†…å®¹å“ˆå¸Œé›†åˆï¼Œç”¨äºé˜²æ­¢å½“å¤©å†…é‡å¤
    today_content_hashes = set()
    today_title_hashes = set()

    # æ¯æ¡æ–°é—»ä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼Œå†…æœ‰index.md
    for idx, article in enumerate(articles):
        title = article.get('title', f'news_{idx+1}')
        summary = article.get('summary', '')
        url = article.get('url', '')
        original_content = article.get('original_content', '')
        tags = article.get('tags', []) # ç›´æ¥ä»JSONè·å–tags

        # --- è¯Šæ–­æ—¥å¿—: å¼€å§‹ ---
        print(f"\n--- æ­£åœ¨å¤„ç†æ–‡ç« : \"{title}\"")
        # --- è¯Šæ–­æ—¥å¿—: ç»“æŸ ---

        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
        title_hash = get_title_hash(title)
        # --- è¯Šæ–­æ—¥å¿—: å¼€å§‹ ---
        print(f"    - æ ‡é¢˜å“ˆå¸Œ: {title_hash}")
        # --- è¯Šæ–­æ—¥å¿—: ç»“æŸ ---
        if title_hash in existing_title_hash_map:
            duplicate_folder = existing_title_hash_map[title_hash]
            print(f"â­ï¸ è·³è¿‡é‡å¤æ ‡é¢˜: {title}")
            print(f"   å·²å­˜åœ¨äº: {duplicate_folder}")
            skipped_articles += 1
            continue
        
        if title_hash in today_title_hashes:
            print(f"â­ï¸ è·³è¿‡å½“å¤©é‡å¤æ ‡é¢˜: {title}")
            skipped_articles += 1
            continue

        # å†…å®¹å»é‡
        content_to_hash = summary + original_content
        content_hash = get_content_hash(content_to_hash)
        # --- è¯Šæ–­æ—¥å¿—: å¼€å§‹ ---
        print(f"    - å†…å®¹å“ˆå¸Œ: {content_hash}")
        # --- è¯Šæ–­æ—¥å¿—: ç»“æŸ ---
        if content_hash in existing_content_hashes:
            print(f"â­ï¸ è·³è¿‡é‡å¤å†…å®¹: {title}")
            skipped_articles += 1
            continue

        if content_hash in today_content_hashes:
            print(f"â­ï¸ è·³è¿‡å½“å¤©é‡å¤å†…å®¹: {title}")
            skipped_articles += 1
            continue

        # ç”Ÿæˆæ›´å¥å£®çš„æ–‡ç« URL slug
        # 1. è½¬ä¸ºå°å†™
        s = title.lower()
        # 2. ç§»é™¤éæ³•å­—ç¬¦ (ä¿ç•™å­—æ¯ã€æ•°å­—ã€- å’Œç©ºæ ¼)
        s = re.sub(r'[^\w\s-]', '', s)
        # 3. å¤šä¸ªç©ºæ ¼æˆ–-æ›¿æ¢ä¸ºå•ä¸ª-
        s = re.sub(r'[\s-]+', '-', s).strip('-')
        # 4. æˆªæ–­
        post_slug = s[:65] # ç¼©çŸ­ä»¥å®¹çº³å‰ç¼€


        # æ·»åŠ æ•°å­—å‰ç¼€
        post_slug_with_prefix = f"{next_article_index:02d}_{post_slug}"

        post_folder = os.path.join(today_folder, post_slug_with_prefix)
        os.makedirs(post_folder, exist_ok=True)
        
        # å°†å½“å‰æ–‡ç« çš„å“ˆå¸Œå€¼åŠ å…¥é›†åˆ
        today_content_hashes.add(content_hash)
        today_title_hashes.add(title_hash)
        
        # åœ¨å­æ–‡ä»¶å¤¹å†…åˆ›å»º index.md
        index_path = os.path.join(post_folder, 'index.md')
        
        # å‡†å¤‡Front Matter
        # ä½¿ç”¨ TOML æ ¼å¼
        # å°†tagsåˆ—è¡¨è½¬æ¢ä¸ºTOMLæ ¼å¼çš„å­—ç¬¦ä¸²æ•°ç»„
        tags_toml = json.dumps(tags)
        
        # ç§»é™¤ "æ‘˜è¦ï¼š" å’Œ "æ ‡ç­¾ï¼š"
        summary_cleaned = summary.replace("æ‘˜è¦ï¼š", "").strip()
        
        front_matter = f"""+++
title = '{title.replace("'", "''")}'
date = "{datetime.now(TARGET_TIMEZONE).isoformat()}"
draft = false
tags = {tags_toml}
summary = "{summary_cleaned.replace('"', '""')[:150]}"
slug = "{post_slug}"
link = "{url}"
+++

{summary_cleaned}

<!--more-->
"""
        
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(front_matter)
            
        print(f"âœ… æˆåŠŸç”Ÿæˆæ–‡ç« : {post_slug_with_prefix}")
        generated_articles += 1
        next_article_index += 1 # ä¸ºä¸‹ä¸€ç¯‡æ–‡ç« å¢åŠ åºå·

    print("\n--- ç”Ÿæˆå®Œæ¯• ---")
    print(f"æ€»å…±å¤„ç†æ–‡ç« : {total_articles}")
    print(f"æˆåŠŸç”Ÿæˆ: {generated_articles}")
    print(f"å› é‡å¤è·³è¿‡: {skipped_articles}")
    print("--- --- ---")

if __name__ == '__main__':
    generate_daily_news_folders() 