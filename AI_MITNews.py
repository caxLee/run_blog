import asyncio
import json
from pathlib import Path
from playwright.async_api import async_playwright
import os

BASE_URL = "https://news.mit.edu"
# ä½¿ç”¨ HUGO_PROJECT_PATH ä»¥ä¾¿åœ¨ GitHub Action ä¸­ä¹Ÿèƒ½è¿è¡Œ
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', r'C:\Users\kongg\0')
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
SAVE_PATH = os.path.join(base_dir, "mit_news_articles.jsonl")
MARKDOWN_PATH = os.path.join(base_dir, "mit_news_articles.md")
HEADLESS = True


def load_existing_urls(path):
    if not Path(path).exists():
        return set()
    with open(path, "r", encoding="utf-8") as f:
        return {json.loads(line)["url"] for line in f if line.strip()}


async def scrape_mit_news_articles(save_path):
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    os.makedirs(os.path.dirname(MARKDOWN_PATH), exist_ok=True)
    existing_urls = load_existing_urls(save_path)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=HEADLESS)
        context = await browser.new_context()

        # åŠ å¿«åŠ è½½é€Ÿåº¦ï¼šæ‹¦æˆªå›¾ç‰‡ã€å­—ä½“ç­‰éå¿…è¦èµ„æº
        await context.route("**/*", lambda route: route.abort()
                            if route.request.resource_type in ["image", "font", "media"]
                            else route.continue_())

        page = await context.new_page()
        print("ğŸ”— æ­£åœ¨è®¿é—® MIT News é¦–é¡µ...")

        # å¢åŠ é‡è¯•é€»è¾‘ï¼Œåº”å¯¹ç½‘ç»œæ³¢åŠ¨
        max_retries = 3
        for attempt in range(max_retries):
            try:
                await page.goto(BASE_URL, timeout=60000)
                print("âœ… æˆåŠŸè®¿é—® MIT News é¦–é¡µã€‚")
                break  # æˆåŠŸï¼Œåˆ™è·³å‡ºå¾ªç¯
            except Exception as e:
                print(f"ğŸ•’ è®¿é—®è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})ï¼Œæ­£åœ¨é‡è¯•...")
                if attempt == max_retries - 1:
                    print(f"âŒ è®¿é—® MIT News å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    await browser.close()
                    return  # é€€å‡ºå‡½æ•°

        print("ğŸ” æ­£åœ¨æå–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥...")
        links = await page.query_selector_all("a.front-page--news-article--teaser--title--link")

        with open(save_path, "a", encoding="utf-8") as f, \
             open(MARKDOWN_PATH, "a", encoding="utf-8") as md_f:
            for link in links:
                try:
                    href = await link.get_attribute("href")
                    title_span = await link.query_selector("span")
                    title = await title_span.inner_text() if title_span else ""

                    if not href or not title.strip():
                        continue  # è·³è¿‡æ— æ ‡é¢˜çš„

                    full_url = BASE_URL + href
                    if full_url in existing_urls:
                        print(f"â­ï¸ å·²æŠ“å–ï¼Œè·³è¿‡ï¼š{full_url}")
                        continue

                    print(f"ğŸ“° æŠ“å–ï¼š{title.strip()}")
                    article_page = await context.new_page()
                    await article_page.goto(full_url, timeout=60000)
                    await article_page.wait_for_selector("div.paragraph--type--content-block-text p", timeout=10000)

                    # è·å–æ­£æ–‡æ®µè½
                    paragraphs = await article_page.locator("div.paragraph--type--content-block-text p").all_inner_texts()
                    content = "\n\n".join(paragraphs)

                    data = {
                        "title": title.strip(),
                        "url": full_url,
                        "content": content.strip()
                    }

                    f.write(json.dumps(data, ensure_ascii=False) + "\n")
                    f.flush()

                    # å†™å…¥Markdown
                    md_f.write(f"## {title.strip()}\n")
                    md_f.write(f"- é“¾æ¥: [{full_url}]({full_url})\n\n")
                    md_f.write(f"**æ­£æ–‡å†…å®¹ï¼š**\n\n{content.strip()}\n\n")
                    md_f.write("---\n\n")
                    md_f.flush()

                    existing_urls.add(full_url)
                    print(f"âœ… å·²ä¿å­˜ï¼š{title.strip()}")
                    await article_page.close()

                except Exception as e:
                    print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        await browser.close()


if __name__ == "__main__":
    asyncio.run(scrape_mit_news_articles(SAVE_PATH))
