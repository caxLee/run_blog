import os
import json
import hashlib
from openai import OpenAI
from tqdm import tqdm
from dotenv import load_dotenv
import sys

# æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ
is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'


load_dotenv()


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")

# ========== åˆå§‹åŒ– ==========
# ä½¿ç”¨ OpenAI v1.x+ å†…ç½®çš„é‡è¯•æœºåˆ¶
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_API_BASE,
    timeout=60.0,  # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
    max_retries=5, # å†…ç½®çš„é‡è¯•æ¬¡æ•°
)


base = None  # SeaTable disabled
table_name = 'AIæ‘˜è¦'  # preserved as placeholder, not used

# --- ç¯å¢ƒè‡ªé€‚åº”çš„æ™ºèƒ½è·¯å¾„é…ç½® ---
hugo_project_path = ''
# é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨ GitHub Actions ç¯å¢ƒä¸­
if os.environ.get('GITHUB_ACTIONS') == 'true':
    print("ğŸ¤– [AI_summary.py] åœ¨ GitHub Actions ä¸­è¿è¡Œ, å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ã€‚")
    hugo_project_path = os.getenv('HUGO_PROJECT_PATH')
    if not hugo_project_path:
        print("âŒ é”™è¯¯: åœ¨ GitHub Actions ç¯å¢ƒä¸­, ç¯å¢ƒå˜é‡ HUGO_PROJECT_PATH æœªè®¾ç½®ã€‚")
        sys.exit(1)
else:
    # å¦‚æœä¸åœ¨äº‘ç«¯ï¼Œåˆ™å‡å®šä¸ºæœ¬åœ°ç¯å¢ƒï¼Œè‡ªåŠ¨è®¡ç®—è·¯å¾„
    print("ğŸ’» [AI_summary.py] åœ¨æœ¬åœ°è¿è¡Œ, å°†è‡ªåŠ¨æ£€æµ‹é¡¹ç›®è·¯å¾„ã€‚")
    hugo_project_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

print(f"âœ… [AI_summary.py] ä½¿ç”¨ Hugo é¡¹ç›®è·¯å¾„: {hugo_project_path}")
# --- è·¯å¾„é…ç½®ç»“æŸ ---

# æ–‡ä»¶è·¯å¾„ç°åœ¨å®Œå…¨åŸºäº hugo_project_path
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
input_files = [
    os.path.join(base_dir, "mit_news_articles.jsonl"),
    os.path.join(base_dir, "jiqizhixin_articles_summarized.jsonl")
]
output_file = os.path.join(base_dir, "summarized_articles.jsonl")
markdown_file = os.path.join(base_dir, "summarized_articles.md")  # æ–°å¢Markdownæ–‡ä»¶å

# ç”¨äºå»é‡çš„å†…å®¹å“ˆå¸Œé›†åˆ
content_hash_set = set()

# ç”¨äºè®¡ç®—å†…å®¹å“ˆå¸Œå€¼
def get_content_hash(content):
    return hashlib.md5(content.encode('utf-8')).hexdigest()

# åŠ è½½å·²æ€»ç»“çš„æ ‡é¢˜é›†åˆå’Œå†…å®¹å“ˆå¸Œé›†åˆ
summarized_titles = set()
if os.path.exists(output_file):
    with open(output_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                summarized_titles.add(data["title"])
                # åŒæ—¶è®°å½•å†…å®¹å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
                if "original_content" in data:
                    content_hash = get_content_hash(data["original_content"])
                    content_hash_set.add(content_hash)
            except:
                continue

# æ·»åŠ é‡è¯•é€»è¾‘
def call_openai_with_retry(model, messages, temperature=0.7, response_format=None):
    """ä½¿ç”¨å†…ç½®é‡è¯•è°ƒç”¨OpenAI API"""
    params = {
        "model": model,
        "messages": messages,
        "temperature": temperature,
    }
    if response_format:
        params["response_format"] = response_format
    return client.chat.completions.create(**params)

# å¤„ç†æ‰€æœ‰è¾“å…¥æ–‡ä»¶
articles = []
for input_file in input_files:
    if not os.path.exists(input_file):
        print(f"âš ï¸ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        continue
    
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                if data.get("title") and data.get("content"):
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                    content_hash = get_content_hash(data["content"])
                    if content_hash in content_hash_set:
                        print(f"â­ï¸ è·³è¿‡é‡å¤å†…å®¹: {data['title']}")
                        continue
                    
                    # å¦‚æœæ˜¯æ–°å†…å®¹ï¼Œåˆ™æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ï¼ŒåŒæ—¶è®°å½•å“ˆå¸Œå€¼
                    articles.append(data)
                    content_hash_set.add(content_hash)
            except Exception as e:
                print(f"âš ï¸ è§£æJSONå¤±è´¥: {e}")

# ç”Ÿæˆæ‘˜è¦
print(f"å¼€å§‹ç”Ÿæˆæ‘˜è¦ï¼Œå…± {len(articles)} ç¯‡ï¼Œå·²æœ‰æ‘˜è¦ {len(summarized_titles)} ç¯‡")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(output_file), exist_ok=True)
os.makedirs(os.path.dirname(markdown_file), exist_ok=True)

# æ’å…¥æ•°æ®å¹¶å†™å…¥ jsonl
with open(output_file, 'a', encoding='utf-8') as out_f, \
     open(markdown_file, 'a', encoding='utf-8') as md_f:  # åŒæ—¶æ‰“å¼€Markdownæ–‡ä»¶
    for article in tqdm(articles, desc="ğŸŒ æ­£åœ¨ç”Ÿæˆæ‘˜è¦"):
        title = article["title"]
        content = article["content"]
        url = article.get("url", "") # è·å–URL
        
        # å¦‚æœæ ‡é¢˜å·²å­˜åœ¨ï¼Œè·³è¿‡
        if title in summarized_titles:
            print(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ ‡é¢˜: {title}")
            continue

        try:
            # å¢åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
            print(f"æ­£åœ¨ä¸ºæ–‡ç«  '{title}' è°ƒç”¨OpenAI APIç”Ÿæˆæ‘˜è¦...")
            # è°ƒç”¨ GPT-3.5 ç”Ÿæˆæ‘˜è¦ï¼ˆå¸¦é‡è¯•ï¼‰
            keywords = ["åŸºæ¨¡", "å¤šæ¨¡æ€", "Infra", "AI4S", "å…·èº«æ™ºèƒ½", "å‚ç›´å¤§æ¨¡å‹", "Agent", "èƒ½æ•ˆä¼˜åŒ–"]
            keywords_str = ", ".join(f'"{k}"' for k in keywords)
            messages = [
                {"role": "system", "content": f"ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ–°é—»ç¼–è¾‘ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»åŸæ–‡ï¼Œå®Œæˆä¸¤é¡¹ä»»åŠ¡ï¼š\n1. **ç”Ÿæˆæ‘˜è¦**: æ’°å†™ä¸€æ®µ3-5å¥è¯çš„ä¸­æ–‡æ‘˜è¦ï¼Œå®¢è§‚ã€å‡†ç¡®åœ°æ¦‚æ‹¬æ–‡ç« çš„æ ¸å¿ƒå†…å®¹ã€‚\n2. **æå–å…³é”®è¯**: ä»ä»¥ä¸‹åˆ—è¡¨ä¸­ç²¾ç¡®é€‰æ‹©1-3ä¸ªæœ€ç›¸å…³çš„å…³é”®è¯ï¼š[{keywords_str}]ã€‚\n\nä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒåŒ…å«ä¸¤ä¸ªé”®ï¼š'summary'ï¼ˆå…¶å€¼ä¸ºæ‘˜è¦å­—ç¬¦ä¸²ï¼‰å’Œ'tags'ï¼ˆå…¶å€¼ä¸ºå…³é”®è¯å­—ç¬¦ä¸²æ•°ç»„ï¼‰ã€‚"},
                {"role": "user", "content": f"æ–°é—»åŸæ–‡ï¼š\n{content}"}
            ]
            response = call_openai_with_retry(
                "gpt-3.5-turbo", 
                messages, 
                temperature=0.5,
                response_format={"type": "json_object"}
            )

            response_data = json.loads(response.choices[0].message.content.strip())
            summary = response_data.get("summary", "")
            tags = response_data.get("tags", [])


            # ä¿å­˜æ‘˜è¦åˆ° jsonl æ–‡ä»¶
            # ä¿å­˜åŸæ–‡é“¾æ¥å’Œå†…å®¹
            article_data = {
                "title": title, 
                "summary": summary,
                "tags": tags,
                "url": url,  # ä¿å­˜åŸæ–‡é“¾æ¥
                "original_content": ""  # ä¸å†ä¿å­˜åŸæ–‡å†…å®¹
            }
            out_f.write(json.dumps(article_data, ensure_ascii=False) + "\n")
            out_f.flush()
            summarized_titles.add(title)
            # å†™å…¥Markdown
            md_f.write(f"## {title}\n\n")
            if url:
                md_f.write(f"**åŸæ–‡é“¾æ¥ï¼š** [{url}]({url})\n\n")
            if tags:
                md_f.write(f"**æ ‡ç­¾ï¼š** {', '.join(tags)}\n\n")
            md_f.write(f"**æ‘˜è¦ï¼š**\n\n{summary}\n\n")
            md_f.write("---\n\n")
            md_f.flush()

            print(f"âœ… æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜æ‘˜è¦: {title}")

        except Exception as e:
            print(f"\nâŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {title}\nåŸå› : {e}")